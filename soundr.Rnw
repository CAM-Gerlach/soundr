% Copyright (C) 2017 C.A.M. Gerlach
%
% This program is free software: you can redistribute it and/or modify
%     it under the terms of the GNU General Public License as published by
%     the Free Software Foundation, either version 3 of the License, or
%     (at your option) any later version.
%
%     This program is distributed in the hope that it will be useful,
%     but WITHOUT ANY WARRANTY; without even the implied warranty of
%     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
%     GNU General Public License for more details.
%
%     You should have received a copy of the GNU General Public License
%     along with this program.  If not, see <http://www.gnu.org/licenses/>.


\documentclass{book}

% Standard imports
\usepackage[utf8]{inputenc} % Cause UTF-8 > Everything else No, seriously.
\usepackage[T1]{fontenc} % Slightly less crappy than to default 128 charset
% Really should be using proper Unicode in xelatex or lualatex
\usepackage{bookmark} % Make PDF bookmarks

% Formatting control
\usepackage{fancyhdr} % Make proper headers
\usepackage[compact]{titlesec} % Make titles/section headers more compact
\usepackage{lmodern} % Change kearning to be more compact, I think
\usepackage{sfmath} % New default math font
%\usepackage{parskip} % Unnessesary with fancier formatting

% Extra features
\usepackage{siunitx} % Make units.
% Acclaimed replacement for the replacement of the 20-year obsolete "units"
\usepackage{amsthm} % Build aisdes on top of, for whatever reason



% Basic page setup and formatting to match Bitzer's

% Use sans font
\renewcommand{\familydefault}{\sfdefault}

% Set margins
\newcommand{\thispagewidth}{6.75in}
\setlength{\topmargin}{0pt} \setlength{\topskip}{0pt}
\setlength{\headheight}{12pt} \setlength{\headsep}{10pt}
\setlength{\headwidth}{\thispagewidth}
\setlength{\textheight}{9in}\setlength{\textwidth}{\thispagewidth}
\setlength{\footskip}{15pt}
\setlength{\hoffset}{0in}\setlength{\voffset}{0in}
\setlength{\marginparwidth}{0.6in}
\setlength{\oddsidemargin}{-.15in}
\setlength{\evensidemargin}{-.15in}
\setlength{\parindent}{0in}
\setlength{\parskip}{20pt}
\renewcommand{\headrulewidth}{1pt}

% Define an "aside" environment, from Bitzer.
% Note: Depends on amsthm
\newtheoremstyle{aside_style}{10pt}{5pt}{\itshape}{}{\bfseries}{:}{\newline}{}
\theoremstyle{aside_style}
\newtheorem*{myaside}{Aside}
\newenvironment{aside}
	{\par\noindent\hrulefill\begin{myaside}}
	{\par\noindent\hrulefill\end{myaside}}

% Make our footers look like Bitzer's.
\pagestyle{fancy} % Obligatory Iggy Azalea joke here
\fancyfoot{}
\fancyfoot[OR, EL]{\thepage}
\fancyfoot[ER, OL]{\tiny{Last edited: \today}}

% Web links
\newcommand{\link}[1]{\href{#1}{\nolinkurl{#1}}}





\begin{document}

% Knitr setup
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
knit_theme$set("camstandardtheme.css")
opts_chunk$set(fig.lp = "figure:", fig.height = 4)
options(knitr.concordance = TRUE)
#render_listings()
@




\chapter{Model Analysis and Verification with R}

Here, we implement a new technique for diagnosing vertical motion, $\omega$, from simulated GOES sounder temperature data, and verify its effectiveness. Further, we seek to illustrate a few of R's data wrangling, analysis, plotting (via \texttt{ggplot2}), and mapping (via \texttt{ggmap}) capabilities, along with some of the tens of thousands of scientific packages avalible for nearly every concievable application



\section{Introduction and Background} \label{intro}

\subsection{Vertical Motion: The good, the bad, and up-ly}

Of all the hundreds of base and derived variables describing the atmopshere's current state, few (if any) are as important—and difficult—to diagnose as vertical motion. Sure, temperature may determine whether or not you bundle up, and (horizontal) winds can make a calm day blustery, but vertical motion (usually referred to as $\omega$ when pressure is the vertical coordinate, and thus measured in \si{Pa.s^{-1}}) is the key to anything actually \textit{happenin'} with the weather, whether that be clouds, rain, snow, storms, or any sort ofmeaningful  \textit{change}. Without it, the weather would be mighly boring all the time!

However, even vertical motions of a few \si{cm.s^{-1}} can be significant on the large (synoptic) scales on which medium and longer range forecasting occurs, and currently, no instruments in common operational use are able to get close to that degree of precision. Accordingly, $\omega$ is typically measured indirectly, via its relationship with other properties, as expressed via the fundamental equations that govern synoptic scale motion in the atmopshere.

The two main elementary approaches, commonly discussed in a typical Dynamics course, are what is termed the "kinematic method" and the "adiabatic method." The first uses the continuity equation in pressure coordinates to show that vertical motion, $\omega$, at a certain level can be related to the divergence, integrated from the surface to that level. While simple in theory, and requiring knowlege of only on the motion fields and density, it is very sensitive to small changes in the wind at various heights, well within the margin of error of most regular measurement techniques, and is therefore not particularly useful in practice.


\subsection{A better approach?}

The second, and the object of our investigation today, is the adiabatic method. We begins with a form of the thermodynamic energy equation (itself derived from  the First Law of Thermodynamics),

\begin{equation} \label{thermoenergyeq}
    \frac{\partial T}{\partial t} + u \frac{\partial T}{\partial x} + v \frac{\partial T}{\partial y} - S_p \omega = \frac{J}{c_p}
\end{equation}

where $S_p$ is a coefficient reprisentatitive of the atmosphere's static stabilty

\[
S_p = - \frac{T}{\theta} \frac{\partial \theta}{\partial P}
\]

If we assume diabatic heating, $J$, is negligable relative to changes in teperature from advection and compression/expansion (thus its name), and simply solve for $\omega$, we have

\begin{equation} \label{omegaeq}
\omega = \frac{1}{S_p} \left(\frac{\partial T}{\partial t} + u \frac{\partial T}{\partial x} + v \frac{\partial T}{\partial y} \right)
\end{equation}

Conceptually, if we assume a closed system (no external heating and cooling), this equation tells that as any temperature changes at a point in 3D space ($\frac{\partial T}{\partial t}$) must be due to either temperature being advected by the prevailing wind from elsewhere (the $u \frac{\partial T}{\partial x}$ and $v \frac{\partial T}{\partial y}$ terms) or be due to cooling (heating) due to air expanding (compressing) as it rises (falls), i.e. decreases (increases) in pressure. Thus, if we subtract the change due to advection from the net change at a point, the remainder maust be due to vertical motion, proportional to it via the stability parameter $S_p$.

While the horizontal winds and temperature gradients involved can be fairly easily obtained, from, e.g. semi-daily radiosonde data, and do not typically change too drastically at the synoptic scale over the course of that period, we have historically had little reliable means of determining the rate of temperature change with time in the middle and upper troposphere, without which the above equation is useless. Radiosondes are only typically released every twelve hours (and even then at a relatively small number of points), which is quite insufficient to get accurate results.

Therefore, this formulation too was ambandoned, eventually to be replaced by much more complex forms under the assumptions of Quasi-Geostrophic theory, that can be more difficult to easily apply and subject to error under the assumptions on which QG theory is based; further, they are generally only calculated from model fields, resulting in further signifiant uncertainties, as no near-real time observational data source is availible that would provide the height and surface pressure tendancy data nessesary for their operational employment.


\subsection{In which we finally get to the point}

However, in recent years, radiometer-based infrared sounders have begun being dployed on geostationary satalite platforms. This, then, could enable the erstwhile-elusive measurement of point temperature change over time, by collecting moderate resolution, high coverage, relatively high frequency temperature data. Presently, such data is collected hourly on a \SI{10}{km} grid by severeal of the GOES-N series satalites, which may already provide some value in this application, while higher frequency observation may be capable of increasing this even further. As a brief survey of the literature does not record a attempt to evaluate a technique of this nature, it would be valuable to examine.

The end product, particularly if sourced from a much more accurate hyperspectral sounder, or in the interm if the new ABI instrument on the GOES-R series platforms is used for higher observational frequency, could enable near real-time graphical products diagnosing $\omega$ from observed data, providing forecasters insight on where to expect the development of clouds and precipiation. As a first step toward this possible goal, due to the unavailibilty of processed temperature data from such soundings, we will instead validate the basic theoretical approach using model data, with various means of increasing realism and "fudge-level" employed to simulate the operational limitations of such sounding instruments, and the results evaluated by several means of verification. Thus, to begin!



\section{Getting Our Data} \label{getData}


\subsection{Defining our domain}

First, we need to retrieve the model data we'll require. While normally this would require some amount of work to interface with the NCEP EMC NOMADS APIs, luckily enough due to the provenance of R, we already have such availible with \texttt{rNOMADS}, the work of the fine R-loving folks at NCAR. For our purposes, let's limit our domain for determining $\omega$ to the CONUS at the 700, 500 and \SI{300}{hPa} pressure levels, over the course of a \SI{24}{\hour} span.

Aside from $\omega$ itself for verification, to solve Equation \ref{omegaeq} we'll need the temperature and $u$- and $v$-winds at each pressure level to determine the temperature gradient, advection, and temperature change over time at each point, as well as the temperatures at levels just above and below (typically, by the minimum increement of \SI{25}{hPa}) to determine how potentential temperature changes with height, needed for the stability parameter $S_p$.

As its \SI{13}{km} resolution closely matches that of the (legacy) GOES sounder, it is based on the widely used WRF-ARW core shared with the HRRR, and is updated with the same frequency as said sounder, the RAP model seems well suited to this application. Ideally, more frequent timesteps would be used, but unfortunatly while the HRRR produces some subhourly output, the non-surface temperature data is not currently included among the output feilds. In the future, a custom WRF run may be needed to accurately similate the needed data.


\subsection{A prelude to battle}

Now that we know what we need, its time to get it. First, we set up our enviroment and select the fields we want:

<<setupenvir, message=FALSE>>=
library(rNOMADS)

# Basic setup
DATA_DIR_PATH = "data"
MODEL_DATA_FILENAME = "model_data_basic"

# Constant parameters controlling data selection
MODEL_TO_USE = "rap"

PRESSURE_LEVELS_HPA = c(700, 500, 300)
PRESSURE_INCREMENT_HPA = 25
PRESSURE_MAX = 1000
PRESSURE_MIN = 100

VARIABLES_NEEDED_LEVELS = c("tmpprs", "ugrdprs", "vgrdprs", "vvelprs")
VARIABLES_NEEDED_VERTICAL = c("tmpprs")
VARIABLES_NEEDED_EXTRA = c("tcdcclm")

TIMES_NEEDED = c(0, 1)

# TODO: Actually compute and round the lat-lon, or retrieve from server
# Non-trivial (source data is on a cartesian grid, but request is in lat lon)
DOMAIN_LON = c(0, 428)
DOMAIN_LAT = c(0, 227)
@

Then, we need to grab the urls and times of the model data we need, using \texttt{rNOMADS}.

<<geturls, cache=TRUE>>=
# Get overall url for most recent day of RAP model data
model.urls <- GetDODSDates(MODEL_TO_USE)
model.url.recent <- tail(model.urls$url, 1)

# Get the most recent model run
model.runs <- GetDODSModelRuns(model.url.recent)

# If you're havin' Perl problems, I feel bad for you, son...
model.runs.fullgrid <- model.runs$model.run[grepl("(rap_[0-2][0-9]z)",
                                                 model.runs$model.run,
                                                 perl = TRUE)]
# I got 99 problems but regex just solved one.
# Got the RAP model on the CAT patrol
# Pilots really don't like flying into those...okay, I'll stop

model.run <- tail(model.runs.fullgrid, 1)
@


\subsection{Getting the goods...almost}

With the preliminaries done, we now need to actually get the data, but a bit more preparation is needed first. Specifically, we need something that will translate our selected pressures into numerical indicies retrieved from the list of model levels, and we want to mash all our levels and variables into two parallel lists so we can get all the data at once.

<<setupdata>>=
# Cute lil' function to get the indicies of the pressure levels we want
get_p_indicies <- function(p.levels)
{
    pressures.all <- seq(PRESSURE_MAX, PRESSURE_MIN,
                         -1 * PRESSURE_INCREMENT_HPA)
    return(which(pressures.all %in% p.levels) - 1)
}


# Construct lists of the levels/variables we want to get, for mapply
levels.combined <- c(list(NULL), lapply(get_p_indicies(PRESSURE_LEVELS_HPA),
                                        function(x) {c(x, x)}),
                     lapply(get_p_indicies(PRESSURE_LEVELS_HPA),
                            function(x) {c(x - 1, x + 1)}))
vars.combined <- c(VARIABLES_NEEDED_EXTRA,
                   rep(list(VARIABLES_NEEDED_LEVELS),
                       length(PRESSURE_LEVELS_HPA)),
                   rep(list(VARIABLES_NEEDED_VERTICAL),
                       length(PRESSURE_LEVELS_HPA)))
times.combined <- unlist(rep(lapply(TIMES_NEEDED, function(x) {list(c(x, x))}),
                      each = length(vars.combined)), recursive = FALSE)
@


\subsection{At long last}

Finally, the moment you've all been waiting for: again using \texttt{rNOMADS}, but this time with some help from good 'ole \texttt{mapply} to get everything done in one fell swoop, without code duplication (and the associated nasty odors...)

<<getdata, eval=FALSE>>=
# Get the various data fields desired
model.data.list <- mapply(DODSGrab,
                          levels = rep(levels.combined, length(TIMES_NEEDED)),
                          variables = rep(vars.combined, length(TIMES_NEEDED)),
                          time = times.combined,
                          MoreArgs = list(model.url = model.url.recent,
                                          model.run = model.run,
                                          # time = TIMES_NEEDED,
                                          lon = DOMAIN_LON,
                                          lat = DOMAIN_LAT,
                                          verbose = TRUE),
                          SIMPLIFY = FALSE)

# Serialize the downloaded data to a file for later use
dir.create(DATA_DIR_PATH, showWarnings = FALSE, recursive = TRUE)
saveRDS(model.data.list,
        file = paste0(DATA_DIR_PATH, "/", MODEL_DATA_FILENAME, ".rds"))
@

We save the data from our long in the tooth efforts, and reload it for future uses (so we don't overtax the poor NCEP servers)

<<loaddata, include=TRUE, cache=TRUE>>=
# (Re)-read the downloaded model data from a file
model.data.list <- readRDS(paste0(DATA_DIR_PATH, "/",
                                    MODEL_DATA_FILENAME, ".rds"))
@

Now, on to the fun part—the omega analysis!



\section{Data Wranglin'} \label{prepareData}

So, now that we have the data we need, we need to transform it into a suitible form for processing and analysis.


\subsection{Nuking cavities}

Looking at our actual data values (e.g. with)

<<head>>=
head(model.data.list[[1]]$value)
@

we notice we have some missing data, flagged as the max floating point values. Before we do anything else, let's take care of those, shall we?

<<removena>>=
# Nuke the NAs
model.data.list <-
    lapply(model.data.list, function(x) {
        x$value[x$value > 1e3 | x$value < -1e3] <- NA; return(x)})
@

Much better, eh?


\subsection{Pulling teeth}

First, we'll extract the model fields into a more easily worked with matrix format. For extra brownie points, since the routine we use is poorly optimized and takes a nontrivial amount of time, we'll parallize it:

<<matrixdata, cache=TRUE, results="hide", cache.rebuild=FALSE>>=
# Setup the compute cluster
library(parallel)
suppressWarnings(file.remove("clusterlog.txt"))
c1 <- makeCluster(detectCores() - 1, outfile = "clusterlog.txt")
clusterEvalQ(c1, library(rNOMADS))

# Transform the downloaded data into an easier to work with grid/matrix format
model.grid.list <- clusterApplyLB(c1, model.data.list, ModelGrid,
                          resolution = c(0.19242568576, 0.185))

stopCluster(c1)
@

Now, we can organize what we've got by pressure level, and then by time, and finally by variable. We'll use some fancy tricks with the apply family of "meta-functions" to avoid for loops.

<<orgnizedata>>=
# Helper function to pull the relevant objects out of a list
select_from_list <- function(match.values, list.tocheck, subitem)
{
    list.tocheck[which(sapply(sapply(list.tocheck, "[[", subitem),
                              function(x, target) {any(match(x, target))},
                              match.values))]
}

# Reorganize the grid list into a logical structure, based around each level
model.grid.levels <- lapply(PRESSURE_LEVELS_HPA, select_from_list,
                            list.tocheck = model.grid.list, subitem = 5)
model.grid.levels <-
    lapply(model.grid.levels,
           function(x) {lapply(TIMES_NEEDED,
                               function(t) {x[((t * 2) + 1):((t * 2) + 2)]})})
@


\subsection{Data, you've been framed}

So, our data structure now sorta-kinda-eh makes sense for raw processing, but what if we want to visualize it in \texttt{ggplot}, or do other niceR things with it? Well, for that, we'll need data frames! Not exactly designed for n-dimensional data, but we can hack it...

<<dataframe>>=
# Takes a model data object and shoves it into a long-format dataframe
model_to_dataframe <- function(model)
{
    data.frame(run.date = model$model.run.date,
               forecast.date = model$forecast.date,
               latitude = model$lat,
               longitude = model$lon,
               level = model$levels,
               variable = model$variables,
               value = model$value,
               stringsAsFactors = FALSE)
}

# Convert our model data to long dfs
model.df.list <- lapply(model.data.list, model_to_dataframe)
@

However, it would be more useful for plotting and analysis if all our data was in a "wide" format, organized by level and lat-lon. Let's do it, with the help of \texttt{tidyr}.

<<tidyr, cache=TRUE, cache.rebuild=FALSE>>=
library(tidyr)

# Wrangle our data frames into a "wide" format
# First, squish by variable
model.df.list <- lapply(model.df.list, spread,
                        key = "variable", value = "value")

# Then, by pressure level for multi level dfs
verticaldfs <- sapply(model.df.list, function(x) {length(unique(x$level)) > 1})
model.df.list[verticaldfs] <- lapply(model.df.list[verticaldfs],
                                     function(x) {
                                         x$temp <- x$level - median(x$level)
                                         x$level <- median(x$level)
                                         return(x)})
model.df.list[verticaldfs] <- lapply(model.df.list[verticaldfs], spread,
                                     key = "temp", value = "tmpprs",
                                     sep = ".")
@

Finally, we want to join our dfs by pressure level.

<<dfmerge, message=FALSE>>=
# Get the levels and dates from each df, remove unnessesary df
df.levels <- sapply(model.df.list, function(x) {x$level[1]})
level.ndf <- "Level not defined"
model.df.sublist <- model.df.list[df.levels != level.ndf]

df.levels <- sapply(model.df.sublist, function(x) {x$level[1]})
df.times <- sapply(model.df.sublist, function(x) {x$forecast.date[1]})

# Join the dfs with similar levels
model.df.levels <- lapply(unique(df.levels[df.levels != level.ndf]),
                          function(cur.level) {
    lapply(unique(df.times), function(cur.time) {
        Reduce(function(...) {merge(...)},
               model.df.sublist[(df.levels == level.ndf |
                                     df.levels == cur.level) &
                                    df.times == cur.time])})})

# Merge the dfs by time
model.df.final <-
    lapply(model.df.levels,
           function(x) {merge(x[[1]], x[[2]], by = c("latitude", "longitude",
                                                    "run.date", "level"))})
@

All done! Whew, that was quite a bit of work.


\subsection{Just a peek}

Having gotten things into shape, we can, for example, view contour plots of some of these data with ggplot, as shown in Figure \ref{figure:basicplots}.

<<basicplots, fig.cap="Plot of some of our RAP fields, at 500 hPa">>=
# library(ggplot2)
# basic_contour_plot <- ggplot()
@


\section{Seeking Omega} \label{processData}




\end{document}
